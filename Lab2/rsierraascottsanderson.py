# -*- coding: utf-8 -*-
"""rsierraascottsanderson.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UKh8D7qCtlInNoYGBqglg1LfV0WAhEPA

#Installing Fortran Magic
"""

pip install -U fortran-magic

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %load_ext fortranmagic

import sys; sys.path.append('..')

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

mpl.rc('figure', figsize=(12, 7))

ran_the_first_cell = True

jan2017 = pd.to_datetime(['2017-01-03 00:00:00+00:00',
 '2017-01-04 00:00:00+00:00',
 '2017-01-05 00:00:00+00:00',
 '2017-01-06 00:00:00+00:00',
 '2017-01-09 00:00:00+00:00',
 '2017-01-10 00:00:00+00:00',
 '2017-01-11 00:00:00+00:00',
 '2017-01-12 00:00:00+00:00',
 '2017-01-13 00:00:00+00:00',
 '2017-01-17 00:00:00+00:00',
 '2017-01-18 00:00:00+00:00',
 '2017-01-19 00:00:00+00:00',
 '2017-01-20 00:00:00+00:00',
 '2017-01-23 00:00:00+00:00',
 '2017-01-24 00:00:00+00:00',
 '2017-01-25 00:00:00+00:00',
 '2017-01-26 00:00:00+00:00',
 '2017-01-27 00:00:00+00:00',
 '2017-01-30 00:00:00+00:00',
 '2017-01-31 00:00:00+00:00',
 '2017-02-01 00:00:00+00:00'])
calendar = jan2017.values.astype('datetime64[D]')

event_dates = pd.to_datetime(['2017-01-06 00:00:00+00:00', 
                             '2017-01-07 00:00:00+00:00', 
                             '2017-01-08 00:00:00+00:00']).values.astype('datetime64[D]')
event_values = np.array([10, 15, 20])

"""<center>
  <h1>The PyData Toolbox</h1>
  <h3>Scott Sanderson (Twitter: @scottbsanderson, GitHub: ssanderson)</h3>
  <h3><a href="https://github.com/ssanderson/pydata-toolbox">https://github.com/ssanderson/pydata-toolbox</a></h3>
</center>

# About Me:

<img src="images/me.jpg" alt="Drawing" style="width: 300px;"/>

- Senior Engineer at [Quantopian](www.quantopian.com)
- Background in Mathematics and Philosophy
- **Twitter:** [@scottbsanderson](https://twitter.com/scottbsanderson)
- **GitHub:** [ssanderson](github.com/ssanderson)

## Outline

- Built-in Data Structures
- Numpy `array`
- Pandas `Series`/`DataFrame`
- Plotting and "Real-World" Analyses

# Data Structures

> Rule 5. Data dominates. If you've chosen the right data structures and organized things well, the algorithms
will almost always be self-evident. Data structures, not algorithms, are central to programming.

- *Notes on Programming in C*, by Rob Pike.

# Lists
"""

assert ran_the_first_cell, "Oh noes!"

l = [1, 'two', 3.0, 4, 5.0, "six"]
l

lista = ['hola', 'esta', 'es', 'una', 'lista']
lista

# Lists can be indexed like C-style arrays.
first = l[0]
second = l[1]
print("first:", first)
print("second:", second)

tercero = lista[2]
cuarto = lista[3]
print("3ro:", tercero)
print("4to:", cuarto)

# Negative indexing gives elements relative to the end of the list.
last = l[-1]
penultimate = l[-2]
print("last:", last)
print("second to last:", penultimate)

print("último:", lista[-1])
print("penúltimo:", lista[-2])

# Lists can also be sliced, which makes a copy of elements between 
# start (inclusive) and stop (exclusive)
sublist = l[1:3]
sublist

sublista = lista[2:4]
sublista

# l[:N] is equivalent to l[0:N].
first_three = l[:3]
first_three

primeros_cuatro = lista[:4]
primeros_cuatro

# l[3:] is equivalent to l[3:len(l)].
after_three = l[3:]
after_three

todos_despues_del_tercero = lista[3:]
todos_despues_del_tercero

# There's also a third parameter, "step", which gets every Nth element.
l = ['a', 'b', 'c', 'd', 'e', 'f', 'g','h']
l[1:7:2]

lista2 = ['h', 'o', 'l', 'a', 's', 's', '3', '4']  
lista2[0:7:2]  #   <- quiere decir, del lista2[0] al lista[7], avanzando de a 2.

# This is a cute way to reverse a list.
l[::-1]

lista2[::-1]

lista2[::-2]  #   <- revesarla avanzando de a 2.

# Lists can be grown efficiently (in O(1) amortized time).
l = [1, 2, 3, 4, 5]
print("Before:", l)
l.append('six')
print("After:", l)

f = [1, 2, 3, 4, 5]
print("Antes:", f)
f.append('')
f.append("<-este es vacío")
print("Después:", f)

# Comprehensions let us perform elementwise computations.
l = [1, 2, 3, 4, 5]
[x * 2 for x in l]

[x * 4 for x in f]

"""## Review: Python Lists

- Zero-indexed sequence of arbitrary Python values.
- Slicing syntax: `l[start:stop:step]` copies elements at regular intervals from `start` to `stop`.
- Efficient (`O(1)`) appends and removes from end.
- Comprehension syntax: `[f(x) for x in l if cond(x)]`.

# Dictionaries
"""

# Dictionaries are key-value mappings.
philosophers = {'David': 'Hume', 'Immanuel': 'Kant', 'Bertrand': 'Russell'}
philosophers

# Like lists, dictionaries are size-mutable.
philosophers['Ludwig'] = 'Wittgenstein'
philosophers

philosophers['Diomedez'] = 'Diaz'
philosophers

del philosophers['David']
philosophers

a = philosophers['Diomedez']
a

# No slicing.
philosophers['Bertrand':'Immanuel']

"""## Review: Python Dictionaries

- Unordered key-value mapping from (almost) arbitrary keys to arbitrary values.
- Efficient (`O(1)`) lookup, insertion, and deletion.
- No slicing (would require a notion of order).

<center><img src="images/pacino.gif" alt="Drawing" style="width: 100%;"/></center>
"""

# Suppose we have some matrices...
a = [[1, 2, 3],
     [2, 3, 4],
     [5, 6, 7],
     [1, 1, 1]]

b = [[1, 2, 3, 4],
     [2, 3, 4, 5]]

def matmul(A, B):
    """Multiply matrix A by matrix B."""
    rows_out = len(A)
    cols_out = len(B[0])
    out = [[0 for col in range(cols_out)] for row in range(rows_out)]
    
    for i in range(rows_out):
        for j in range(cols_out):
            for k in range(len(B)):
                out[i][j] += A[i][k] * B[k][j]
    return out

"""<center><img src="images/gross.gif" alt="Drawing" style="width: 50%;"/></center>

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# matmul(a, b)

import random
def random_matrix(m, n):
    out = []
    for row in range(m):
        out.append([random.random() for col in range(n)])
    return out

randm = random_matrix(2, 3)
randm

matriz_aleatoria1 = random_matrix(0,0)

matriz_aleatoria1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# randa = random_matrix(600, 100)
# randb = random_matrix(100, 600)
# x = matmul(randa, randb)

# Maybe that's not that bad?  Let's try a simpler case.
def python_dot_product(xs, ys):
    return sum(x * y for x, y in zip(xs, ys))

# Commented out IPython magic to ensure Python compatibility.
# %%fortran
# subroutine fortran_dot_product(xs, ys, result)
#     double precision, intent(in) :: xs(:)
#     double precision, intent(in) :: ys(:)
#     double precision, intent(out) :: result
#     
#     result = sum(xs * ys)
# end

list_data = [float(i) for i in range(100000)]
array_data = np.array(list_data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# python_dot_product(list_data, list_data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# fortran_dot_product(array_data, array_data)

"""<center><img src="images/sloth.gif" alt="Drawing" style="width: 1080px;"/></center>

## Why is the Python Version so Much Slower?
"""

# Dynamic typing.
def mul_elemwise(xs, ys):
    return [x * y for x, y in zip(xs, ys)]

mul_elemwise([1, 2, 3, 4], [1, 2 + 0j, 3.0, 'four'])
#[type(x) for x in _]

# Interpretation overhead.
source_code = 'a + b * c'
bytecode = compile(source_code, '', 'eval')
import dis; dis.dis(bytecode)

"""## Why is the Python Version so Slow?
- Dynamic typing means that every single operation requires dispatching on the input type.
- Having an interpreter means that every instruction is fetched and dispatched at runtime.
- Other overheads:
  - Arbitrary-size integers.
  - Reference-counted garbage collection.

> This is the paradox that we have to work with when we're doing scientific or numerically-intensive Python. What makes Python fast for development -- this high-level, interpreted, and dynamically-typed aspect of the language -- is exactly what makes it slow for code execution.

- Jake VanderPlas, [*Losing Your Loops: Fast Numerical Computing with NumPy*](https://www.youtube.com/watch?v=EEUXKG97YRw)

# What Do We Do?

<center><img src="images/runaway.gif" alt="Drawing" style="width: 50%;"/></center>

<center><img src="images/thisisfine.gif" alt="Drawing" style="width: 1080px;"/></center>

- Python is slow for numerical computation because it performs dynamic dispatch on every operation we perform...

- ...but often, we just want to do the same thing over and over in a loop!

- If we don't need Python's dynamicism, we don't want to pay (much) for it.

- **Idea:** Dispatch **once per operation** instead of **once per element**.
"""

import numpy as np

data = np.array([1, 2, 3, 4])
data

data + data

data + data + data + data

4 * data

(data + data + data + data)  -  (4 * data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Naive dot product
# (array_data * array_data).sum()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Built-in dot product.
# array_data.dot(array_data)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# fortran_dot_product(array_data, array_data)

# Numpy won't allow us to write a string into an int array.
data[0] = "foo"

# We also can't grow an array once it's created.
data.append(3)

# We **can** reshape an array though.
two_by_two = data.reshape(2, 2)
two_by_two

"""Numpy arrays are:

- Fixed-type

- Size-immutable

- Multi-dimensional

- Fast\*

\* If you use them correctly.

# What's in an Array?
"""

arr = np.array([1, 2, 3, 4, 5, 6], dtype='int16').reshape(2, 3)
print("Array:\n", arr, sep='')
print("===========")
print("DType:", arr.dtype)
print("Shape:", arr.shape)
print("Strides:", arr.strides)
print("Data:", arr.data.tobytes())

"""# Core Operations

- Vectorized **ufuncs** for elementwise operations.
- Fancy indexing and masking for selection and filtering.
- Aggregations across axes.
- Broadcasting

# UFuncs

UFuncs (universal functions) are functions that operate elementwise on one or more arrays.
"""

data = np.arange(15).reshape(3, 5)
data

# Binary operators.
data * data

# Unary functions.
np.sqrt(data)

# Comparison operations
(data % 3) == 0

data <3     #   <-   XD

# Boolean combinators.
((data % 2) == 0) & ((data % 3) == 0)

# as of python 3.5, @ is matrix-multiply
data @ data.T

"""# UFuncs Review

- UFuncs provide efficient elementwise operations applied across one or more arrays.
- Arithmetic Operators (`+`, `*`, `/`)
- Comparisons (`==`, `>`, `!=`)
- Boolean Operators (`&`, `|`, `^`)
- Trigonometric Functions (`sin`, `cos`)
- Transcendental Functions (`exp`, `log`)

# Selections

We often want to perform an operation on just a subset of our data.
"""

sines = np.sin(np.linspace(0, 3.14, 10))
cosines = np.cos(np.linspace(0, 3.14, 10))
sines

# Slicing works with the same semantics as Python lists.
sines[0]

sines[:3]  # First three elements

sines[:]    #cuando no se pone nada, el slicing da del primero al último

sines[999999:999999]    #cuando se ponen iguales en la notación del slicing, da el tipo de arreglo

sines[5:]  # Elements from 5 on.

sines[9:]

sines[0:]

sines[10:] #cuando se pone un inicio que sobrepasa el límite, da otra vez información del arreglo

sines[::2]  # Every other element.

# More interesting: we can index with boolean arrays to filter by a predicate.
print("sines:\n", sines)
print("sines > 0.5:\n", sines > 0.5)      #    <-   tells if condition is satisfied
print("sines[sines > 0.5]:\n", sines[sines > 0.5])   #     <-   makes an array from the values that satisfy the condition  OMG

print("sines[sines <= 0.5]:\n", sines[sines <= 0.5])

# We index with lists/arrays of integers to select values at those indices.
print(sines)
sines[[0, 4, 7]]

# Index arrays are often used for sorting one or more arrays.
unsorted_data = np.array([1, 3, 2, 12, -1, 5, 2])

sort_indices = np.argsort(unsorted_data)
sort_indices

unsorted_data[sort_indices]

market_caps = np.array([12, 6, 10, 5, 6])  # Presumably in dollars?
assets = np.array(['A', 'B', 'C', 'D', 'E'])

# Sort assets by market cap by using the permutation that would sort market caps on ``assets``.
sort_by_mcap = np.argsort(market_caps)
assets[sort_by_mcap]

# Indexers are also useful for aligning data.
print("Dates:\n", repr(event_dates))
print("Values:\n", repr(event_values))
print("Calendar:\n", repr(calendar))

print("Raw Dates:", event_dates)
print("Indices:", calendar.searchsorted(event_dates))
print("Forward-Filled Dates:", calendar[calendar.searchsorted(event_dates)])

"""On multi-dimensional arrays, we can slice along each axis independently."""

data = np.arange(25).reshape(5, 5)
data

data[:2, :2]  # First two rows and first two columns.

data[2:, 2:]  #después de la fila 2 y la columna 2

data[:2, [0, -1]]  # First two rows, first and last columns.

data[(data[:, 0] % 2) == 0]  # Rows where the first column is divisible by two.

"""# Selections Review

- Indexing with an integer removes a dimension.
- Slicing operations work on Numpy arrays the same way they do on lists.
- Indexing with a boolean array filters to True locations.
- Indexing with an integer array selects indices along an axis.
- Multidimensional arrays can apply selections independently along different axes.

## Reductions

Functions that reduce an array to a scalar.

$Var(X) = \frac{1}{N}\sqrt{\sum_{i=1}^N (x_i - \bar{x})^2}$
"""

def variance(x):
    return ((x - x.mean()) ** 2).sum() / len(x)

variance(np.random.standard_normal(1000))

"""- `sum()` and `mean()` are both **reductions**.

- In the simplest case, we use these to reduce an entire array into a single value...
"""

data = np.arange(30)
data.mean()

"""- ...but we can do more interesting things with multi-dimensional arrays."""

data = np.arange(30).reshape(3, 10)
data

data.mean()

data = data.reshape(5, 6)  # re-arreglarlo a una matriz de 5 x 6
data

data.mean()

data.mean(axis=0)

data.mean(axis=1)

"""## Reductions Review

- Reductions allow us to perform efficient aggregations over arrays.
- We can do aggregations over a single axis to collapse a single dimension.
- Many built-in reductions (`mean`, `sum`, `min`, `max`, `median`, ...).

# Broadcasting
"""

row = np.array([1, 2, 3, 4])
column = np.array([[1], [2], [3]])
print("Row:\n", row, sep='')
print("Column:\n", column, sep='')

row + column

"""<center><img src="images/broadcasting.png" alt="Drawing" style="width: 60%;"/></center>

<h5>Source: http://www.scipy-lectures.org/_images/numpy_broadcasting.png</h5>
"""

# Broadcasting is particularly useful in conjunction with reductions.
print("Data:\n", data, sep='')
print("Mean:\n", data.mean(axis=0), sep='')
print("Data - Mean:\n", data - data.mean(axis=0), sep='')

"""# Broadcasting Review

- Numpy operations can work on arrays of different dimensions as long as the arrays' shapes are still "compatible".
- Broadcasting works by "tiling" the smaller array along the missing dimension.
- The result of a broadcasted operation is always at least as large in each dimension as the largest array in that dimension.

# Numpy Review

- Numerical algorithms are slow in pure Python because the overhead dynamic dispatch dominates our runtime.

- Numpy solves this problem by:
  1. Imposing additional restrictions on the contents of arrays.
  2. Moving the inner loops of our algorithms into compiled C code.

- Using Numpy effectively often requires reworking an algorithms to use vectorized operations instead of for-loops, but the resulting operations are usually simpler, clearer, and faster than the pure Python equivalent.

<center><img src="images/unicorn.jpg" alt="Drawing" style="width: 75%;"/></center>

Numpy is great for many things, but...

- Sometimes our data is equipped with a natural set of **labels**:
  - Dates/Times
  - Stock Tickers
  - Field Names (e.g. Open/High/Low/Close)

- Sometimes we have **more than one type of data** that we want to keep grouped together.
  - Tables with a mix of real-valued and categorical data.

- Sometimes we have **missing** data, which we need to ignore, fill, or otherwise work around.

<center><img src="images/panda-wrangling.gif" alt="Drawing" style="width: 75%;"/></center>

<center><img src="images/pandas_logo.png" alt="Drawing" style="width: 75%;"/></center>

Pandas extends Numpy with more complex data structures:

- `Series`: 1-dimensional, homogenously-typed, labelled array.
- `DataFrame`: 2-dimensional, semi-homogenous, labelled table.

Pandas also provides many utilities for: 
- Input/Output
- Data Cleaning
- Rolling Algorithms
- Plotting

# Selection in Pandas
"""

import pandas as pd
s = pd.Series(index=['a', 'b', 'c', 'd', 'e'], data=[1, 2, 3, 4, 5])
s

g = pd.Series(index=['k', 'a', 'l', 'o', 's', 't', 'r', 'o'], data=[1, 2, 3, 4, 5, 6, 7, 8])
g

# There are two pieces to a Series: the index and the values.
print("The index is:", s.index)
print("The values are:", s.values)

print("The index is:", g.index)
print("The values are:", g.values)

# We can look up values out of a Series by position...
s.iloc[0]

g.iloc[5]

# ... or by label.
s.loc['a']

g.loc['s']

# Slicing works as expected...
s.iloc[:2]

# ...but it works with labels too!
s.loc[:'c']

# Fancy indexing works the same as in numpy.
s.iloc[[0, -1]]

# As does boolean masking.
s.loc[s > 2]

# Element-wise operations are aligned by index.
other_s = pd.Series({'a': 10.0, 'c': 20.0, 'd': 30.0, 'z': 40.0})
other_s

s + other_s   #   <-   since EW are index aligned, + only adds els. with the same index.

# We can fill in missing values with fillna().
(s + other_s).fillna(0.0)

# Most real datasets are read in from an external file format.
aapl = pd.read_csv('/content/sample_data/AAPL.csv', parse_dates=['Date'], index_col='Low')
aapl.head()

aapl = pd.read_csv('/content/sample_data/AAPL.csv', parse_dates=['Date'], index_col='Date')  #  <- Sobreescribir "aapl" con "date" como índice
aapl.head()

# Slicing generalizes to two dimensions as you'd expect:
aapl.iloc[:2, :2]

aapl.iloc[2:, 2:]

aapl.loc[pd.Timestamp('2010-02-01'):pd.Timestamp('2010-02-04'), ['Close', 'Volume']]

"""# Rolling Operations

<center><img src="images/rolling.gif" alt="Drawing" style="width: 75%;"/></center>
"""

aapl.rolling(5)[['Close', 'Adj Close']].mean().plot();

aapl.rolling(10)[['High', 'Low']].mean().plot();

# Drop `Volume`, since it's way bigger than everything else.
aapl.drop('Volume', axis=1).resample('2W').max().plot();

# 30-day rolling exponentially-weighted stddev of returns.
aapl['Close'].pct_change().ewm(span=30).std().plot();

"""# "Real World" Data"""

from demos.avocados import read_avocadata

avocados = read_avocadata('2014', '2016')
avocados.head()

# Unlike numpy arrays, pandas DataFrames can have a different dtype for each column.
avocados.dtypes

# What's the regional average price of a HASS avocado every day?
hass = avocados[avocados.Variety == 'HASS']
hass.groupby(['Date', 'Region'])['Weighted Avg Price'].mean().unstack().ffill().plot();

def _organic_spread(group):

    if len(group.columns) != 2:
        return pd.Series(index=group.index, data=0.0)
    
    is_organic = group.columns.get_level_values('Organic').values.astype(bool)
    organics = group.loc[:, is_organic].squeeze()
    non_organics = group.loc[:, ~is_organic].squeeze()
    diff = organics - non_organics
    return diff

def organic_spread_by_region(df):
    """What's the difference between the price of an organic 
    and non-organic avocado within each region?
    """
    return (
        df
        .set_index(['Date', 'Region', 'Organic'])
         ['Weighted Avg Price']
        .unstack(level=['Region', 'Organic'])
        .ffill()
        .groupby(level='Region', axis=1)
        .apply(_organic_spread)
    )

organic_spread_by_region(hass).plot();
plt.gca().set_title("Daily Regional Organic Spread");
plt.legend(bbox_to_anchor=(1, 1));

spread_correlation = organic_spread_by_region(hass).corr()
spread_correlation

import seaborn as sns
grid = sns.clustermap(spread_correlation, annot=True)
fig = grid.fig
axes = fig.axes
ax = axes[2]
ax.set_xticklabels(ax.get_xticklabels(), rotation=45);

"""# Pandas Review

- Pandas extends numpy with more complex datastructures and algorithms.
- If you understand numpy, you understand 90% of pandas.
- `groupby`, `set_index`, and `unstack` are powerful tools for working with categorical data.
- Avocado prices are surprisingly interesting :)

# Thanks!
"""